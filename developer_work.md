[Back to home](https://venkatvv.github.io/)

# Overview 

I created a data pipeline for the Professor Z. Morley Mao over the summer of 2016. It was created by utilizing Apache Kafka (a distributed streaming platform) and Apache Hadoop (a framework that allows for the distributed processing of large data sets across clusters of computers). The data is then displayed visually through Grafana (a time series database) and shown in real-time.

This presentation below provides a brief overview of my work for the University of Michigan.

<iframe id="iframe_container" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="100%" height="510" src="https://prezi.com/embed/4zdlawrgpltz/?bgcolor=ffffff&amp;lock_to_path=0&amp;autoplay=0&amp;autohide_ctrls=0&amp;landing_data=bHVZZmNaNDBIWnNjdEVENDRhZDFNZGNIUE43MHdLNWpsdFJLb2ZHanI5bHdQaWVXZ0ZUdG5Bb3JTbk9HL3JNbTN3PT0&amp;landing_sign=mdX47eiGRdRTVxFZVnMsyUjb29DJcnPT3cJ3kq0iHDc"></iframe>

# My Contributions and Experience 

Coming into this job I had very little experience in working on projects without any types of guidelines. I was given a list of requirements and a timeframe to complete the project and was not given any other advice on how to accomplish this task. 

To start this project I first gathered information on all the technology services offered by the University of Michigan. From there my co-worker and I decided on a prototype design for the project. We created a small local environment to implement and test the viability of our design. We then implemented in on a larger scale with the University's servers and other resources available to us. 

This job taught me how to work resourcefully as a developer as well as teaching me the procedures and methods I should take before I start developing a project. I learned a lot from this job and it gave me a lot valuable experience in learning how to prototype, design, and implement software for unique tasks.
